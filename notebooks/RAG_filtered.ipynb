{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d488cb-8c54-4177-8d80-d3bb6ef323e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: openpyxl in /home/mlcore/conda/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: pandas in /home/mlcore/conda/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/mlcore/conda/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: et-xmlfile in /home/mlcore/conda/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mlcore/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mlcore/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mlcore/conda/lib/python3.10/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/mlcore/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f399f7-a595-4b07-b733-bb6367941289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/conda/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "channels = pd.read_excel(\"../data/tg_channels.xlsx\")\n",
    "#df = pd.read_csv(\"../data/cleaned_news_exp.csv\")[[\"message_id\", \"id_channel\", \"message\", \"date\", \"topic\"]]\n",
    "df = pd.read_parquet(\"../data/tg_news_full.parquet\")[[\"message_id\", \"id_channel\", \"message\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf2a27c-2557-4aef-acc6-bfdf9bea8481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>id_channel</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>channel_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275548</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –°–ª–æ–≤–∞–∫–∏–∏ –æ–±—Å—É–¥–∏—Ç –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º...</td>\n",
       "      <td>2025-01-02 17:00:02</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275547</td>\n",
       "      <td>3</td>\n",
       "      <td>–í –î–¢–ü —Å —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ–±—É—Å–æ–º –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –ø–æ—Å...</td>\n",
       "      <td>2025-01-02 16:40:53</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275546</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü—Ä–µ–º—å–µ—Ä –ò–∑—Ä–∞–∏–ª—è –ù–µ—Ç–∞–Ω—å—è—Ö—É –≤—ã–ø–∏—Å–∞–Ω –∏–∑ –±–æ–ª—å–Ω–∏—Ü—ã ...</td>\n",
       "      <td>2025-01-02 16:20:12</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>275545</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –≤ –ø–æ–¥—Ä—ã–≤–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è Tesla Cyber...</td>\n",
       "      <td>2025-01-02 15:54:29</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275543</td>\n",
       "      <td>3</td>\n",
       "      <td>–°–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è. –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ ...</td>\n",
       "      <td>2025-01-02 15:32:55</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   message_id  id_channel                                            message  \\\n",
       "0      275548           3  –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –°–ª–æ–≤–∞–∫–∏–∏ –æ–±—Å—É–¥–∏—Ç –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º...   \n",
       "1      275547           3  –í –î–¢–ü —Å —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ–±—É—Å–æ–º –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –ø–æ—Å...   \n",
       "2      275546           3  –ü—Ä–µ–º—å–µ—Ä –ò–∑—Ä–∞–∏–ª—è –ù–µ—Ç–∞–Ω—å—è—Ö—É –≤—ã–ø–∏—Å–∞–Ω –∏–∑ –±–æ–ª—å–Ω–∏—Ü—ã ...   \n",
       "3      275545           3  –ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –≤ –ø–æ–¥—Ä—ã–≤–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è Tesla Cyber...   \n",
       "4      275543           3  –°–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è. –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ ...   \n",
       "\n",
       "                 date channel_name  \n",
       "0 2025-01-02 17:00:02  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  \n",
       "1 2025-01-02 16:40:53  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  \n",
       "2 2025-01-02 16:20:12  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  \n",
       "3 2025-01-02 15:54:29  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  \n",
       "4 2025-01-02 15:32:55  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_map = (channels[[\"id\", \"name\"]]\n",
    "          .dropna()\n",
    "          .assign(id=lambda x: pd.to_numeric(x[\"id\"], errors=\"coerce\"))\n",
    "          .dropna(subset=[\"id\"])\n",
    "          .assign(id=lambda x: x[\"id\"].astype(int))\n",
    "          .set_index(\"id\")[\"name\"]\n",
    "          .to_dict())\n",
    "\n",
    "df = df.copy()\n",
    "df[\"id_channel\"] = pd.to_numeric(df[\"id_channel\"], errors=\"coerce\")\n",
    "df[\"channel_name\"] = df[\"id_channel\"].map(ch_map).fillna(df[\"id_channel\"].astype(\"Int64\").astype(str))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d52ac2-bde7-4917-87f2-c3f26a844a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_news_text(t: str) -> str:\n",
    "    t = t or \"\"\n",
    "    t = re.sub(r\"#\\w+\", \" \", t)\n",
    "    t = re.sub(r\"[‚ö°Ô∏èüìàüìâüá∑üá∫‚úÖ‚ùóÔ∏èüî•‚¨õ ‚¨ú ‚ö´ ‚ö™üîπ]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, col: str = \"date\") -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[col] = pd.to_datetime(d[col], utc=True, errors=\"coerce\")\n",
    "    d = d.dropna(subset=[col])\n",
    "    d[\"date_day\"] = d[col].dt.floor(\"D\")\n",
    "    return d\n",
    "\n",
    "df = ensure_datetime(df, \"date\")\n",
    "df[\"message_id\"] = df[\"message_id\"].astype(str)\n",
    "df[\"message\"] = df[\"message\"].fillna(\"\").astype(str).map(clean_news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b05d5ab-1cbd-4a6d-8f62-825f02d0456b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>id_channel</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275548</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –°–ª–æ–≤–∞–∫–∏–∏ –æ–±—Å—É–¥–∏—Ç –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º...</td>\n",
       "      <td>2025-01-02 17:00:02+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275547</td>\n",
       "      <td>3</td>\n",
       "      <td>–í –î–¢–ü —Å —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ–±—É—Å–æ–º –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –ø–æ—Å...</td>\n",
       "      <td>2025-01-02 16:40:53+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275546</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü—Ä–µ–º—å–µ—Ä –ò–∑—Ä–∞–∏–ª—è –ù–µ—Ç–∞–Ω—å—è—Ö—É –≤—ã–ø–∏—Å–∞–Ω –∏–∑ –±–æ–ª—å–Ω–∏—Ü—ã ...</td>\n",
       "      <td>2025-01-02 16:20:12+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>275545</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –≤ –ø–æ–¥—Ä—ã–≤–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è Tesla Cyber...</td>\n",
       "      <td>2025-01-02 15:54:29+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275543</td>\n",
       "      <td>3</td>\n",
       "      <td>–°–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è. –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ ...</td>\n",
       "      <td>2025-01-02 15:32:55+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  message_id  id_channel                                            message  \\\n",
       "0     275548           3  –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –°–ª–æ–≤–∞–∫–∏–∏ –æ–±—Å—É–¥–∏—Ç –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º...   \n",
       "1     275547           3  –í –î–¢–ü —Å —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ–±—É—Å–æ–º –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –ø–æ—Å...   \n",
       "2     275546           3  –ü—Ä–µ–º—å–µ—Ä –ò–∑—Ä–∞–∏–ª—è –ù–µ—Ç–∞–Ω—å—è—Ö—É –≤—ã–ø–∏—Å–∞–Ω –∏–∑ –±–æ–ª—å–Ω–∏—Ü—ã ...   \n",
       "3     275545           3  –ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –≤ –ø–æ–¥—Ä—ã–≤–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è Tesla Cyber...   \n",
       "4     275543           3  –°–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è. –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ ...   \n",
       "\n",
       "                       date channel_name                  date_day  \n",
       "0 2025-01-02 17:00:02+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  \n",
       "1 2025-01-02 16:40:53+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  \n",
       "2 2025-01-02 16:20:12+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  \n",
       "3 2025-01-02 15:54:29+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  \n",
       "4 2025-01-02 15:32:55+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82283b6-cbae-483e-8495-f4170eed9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77a872d-7698-435d-a369-479b7b1e534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def tokenize_ru(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^0-9a-z–∞-—è—ë\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n",
    "\n",
    "corpus_tok = [tokenize_ru(t) for t in df[\"message\"].tolist()]\n",
    "bm25 = BM25Okapi(corpus_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ea3567-02fb-40d0-a13e-e77de60fe2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install faiss-cpu faiss-gpu-cu12 faiss-gpu-cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4290f08a-82d8-40b6-af73-e89d0386bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-large\" \n",
    "encoder = SentenceTransformer(MODEL_NAME, device=\"cuda\")  \n",
    "texts = df[\"message\"].tolist()\n",
    "doc_inputs = [\"passage: \" + t for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7436ed1c-6f09-413b-ae3b-03d76af07d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E_docs = encoder.encode(\n",
    "#     doc_inputs,\n",
    "#     batch_size=64,\n",
    "#     show_progress_bar=True,\n",
    "#     normalize_embeddings=True,\n",
    "# ).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb37ea91-457f-407e-bff0-7e743bf4430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "\n",
    "# dim = E_docs.shape[1]\n",
    "# index = faiss.IndexFlatIP(dim)      \n",
    "# index.add(E_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab301b0c-3979-48dd-97f9-c3c16f713b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import faiss\n",
    "# import pickle\n",
    "\n",
    "# OUT = Path(\"indexes\")\n",
    "# OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# #rowmap\n",
    "# rowmap = df[[\"message_id\",\"date\",\"date_day\",\"id_channel\",\"channel_name\"]].copy()\n",
    "# rowmap.to_parquet(OUT / \"rowmap.parquet\", index=False)\n",
    "\n",
    "# np.save(OUT / \"E_docs_e5_large.npy\", E_docs)\n",
    "\n",
    "# faiss.write_index(index, str(OUT / \"faiss_e5_large.index\"))\n",
    "\n",
    "# with open(OUT / \"bm25_corpus_tok.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(corpus_tok, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c4f45-33ca-49b9-b735-599e5d851055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2760a8-4f82-4159-899f-e94447a94e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "INP = Path(\"indexes\")\n",
    "\n",
    "rowmap = pd.read_parquet(INP / \"rowmap.parquet\")\n",
    "\n",
    "E_docs = np.load(INP / \"E_docs_e5_large.npy\")\n",
    "index = faiss.read_index(str(INP / \"faiss_e5_large.index\"))\n",
    "\n",
    "with open(INP / \"bm25_corpus_tok.pkl\", \"rb\") as f:\n",
    "    corpus_tok = pickle.load(f)\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(corpus_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcef3d8f-f6d5-4ded-b27d-b86e1b53b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rowmap) == E_docs.shape[0]\n",
    "assert len(rowmap) == index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17969e2a-8bfc-414a-86a7-da15a18cfedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde85e14-f892-47a2-b585-4af1c1b8b5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f76acc5-fc79-4909-bfdd-c4211312fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def snippet(t: str, n: int = 1000) -> str:\n",
    "    return t[:n]\n",
    "\n",
    "def _topk_indices_from_scores(scores: np.ndarray, k: int) -> np.ndarray:\n",
    "    k = min(k, len(scores))\n",
    "    if k <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if k == len(scores):\n",
    "        idx = np.argsort(-scores)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores, k-1)[:k]\n",
    "        idx = idx[np.argsort(-scores[idx])]\n",
    "    return idx.astype(int)\n",
    "\n",
    "def dense_candidates_faiss(index, encoder, query: str, topN: int = 500):\n",
    "    qv = encoder.encode([\"query: \" + query], normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
    "    scores, idx = index.search(qv, topN) \n",
    "    return idx[0].astype(int), scores[0].astype(np.float32)\n",
    "\n",
    "\n",
    "def hybrid_retrieve_rrf(\n",
    "    df: pd.DataFrame,\n",
    "    index,\n",
    "    encoder,\n",
    "    bm25,\n",
    "    tokenize_fn,\n",
    "    query: str,\n",
    "    k: int = 50,\n",
    "    topN_each: int = 500,\n",
    "    k_rrf: int = 60,\n",
    "    w_dense: float = 1.0,\n",
    "    w_bm25: float = 1.0,\n",
    "    exclude_message_id: str | None = None,\n",
    "    anchor_date: str | pd.Timestamp | None = None,\n",
    "    date_col: str = \"date_day\") -> pd.DataFrame:\n",
    "\n",
    "    if anchor_date is not None:\n",
    "        ad = pd.to_datetime(anchor_date, utc=True).normalize()\n",
    "\n",
    "        if date_col not in df.columns:\n",
    "            raise KeyError(f\"date_col='{date_col}' not found in df.columns\")\n",
    "\n",
    "        dts = pd.to_datetime(df[date_col], errors=\"coerce\", utc=True).dt.normalize()\n",
    "        allowed = (dts <= ad)\n",
    "        allowed_np = allowed.to_numpy(dtype=bool)\n",
    "    else:\n",
    "        allowed_np = None\n",
    "\n",
    "    d_idx, d_sc = dense_candidates_faiss(index, encoder, query, topN=topN_each)\n",
    "\n",
    "    if allowed_np is not None:\n",
    "        keep = allowed_np[d_idx]\n",
    "        d_idx = d_idx[keep]\n",
    "        d_sc = d_sc[keep]\n",
    "\n",
    "    dense_rank = {int(rowpos): r for r, rowpos in enumerate(d_idx, start=1)}\n",
    "\n",
    "    if bm25 is None:\n",
    "        out = df.iloc[d_idx].copy()\n",
    "        out[\"_rowpos\"] = d_idx\n",
    "        out[\"score_rrf\"] = (w_dense / (k_rrf + out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan)).astype(np.float32)))\n",
    "        out[\"rank_dense\"] = out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan))\n",
    "        out[\"rank_bm25\"] = np.nan\n",
    "\n",
    "        if exclude_message_id is not None and \"message_id\" in out.columns:\n",
    "            out = out[out[\"message_id\"].astype(str) != str(exclude_message_id)]\n",
    "\n",
    "        return out.head(k).reset_index(drop=True)\n",
    "\n",
    "    bm_scores = bm25.get_scores(tokenize_fn(query)).astype(np.float32)\n",
    "    if allowed_np is not None:\n",
    "        bm_scores[~allowed_np] = -np.inf\n",
    "\n",
    "    b_idx = _topk_indices_from_scores(bm_scores, topN_each)\n",
    "    bm_rank = {int(rowpos): r for r, rowpos in enumerate(b_idx, start=1)}\n",
    "\n",
    "    union = np.array(sorted(set(dense_rank) | set(bm_rank)), dtype=int)\n",
    "    rrf = np.zeros(len(union), dtype=np.float32)\n",
    "\n",
    "    for j, rowpos in enumerate(union):\n",
    "        if rowpos in dense_rank:\n",
    "            rrf[j] += w_dense / (k_rrf + dense_rank[rowpos])\n",
    "        if rowpos in bm_rank:\n",
    "            rrf[j] += w_bm25 / (k_rrf + bm_rank[rowpos])\n",
    "\n",
    "    order = np.argsort(-rrf)\n",
    "    union = union[order]\n",
    "    rrf = rrf[order]\n",
    "\n",
    "    out = df.iloc[union].copy()\n",
    "    out[\"_rowpos\"] = union\n",
    "    out[\"score_rrf\"] = rrf\n",
    "    out[\"rank_dense\"] = out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan))\n",
    "    out[\"rank_bm25\"] = out[\"_rowpos\"].map(lambda rp: bm_rank.get(int(rp), np.nan))\n",
    "\n",
    "    if exclude_message_id is not None and \"message_id\" in out.columns:\n",
    "        out = out[out[\"message_id\"].astype(str) != str(exclude_message_id)]\n",
    "\n",
    "    return out.head(k).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149ac8fd-e146-4487-8cbb-3731554be0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-09-23 00:00:00+0000', tz='UTC'),\n",
       " Timestamp('2025-09-08 00:00:00+0000', tz='UTC'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"date_day\"].min(), df[\"date_day\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc92482-2a6e-4757-a3a7-6b9a96e0c03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b39a2-ec1c-4e6b-8b3d-8878c8c8a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0464504d-eaa9-480f-bb2f-a62c2e868ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers torch sentence-transformers accelerate vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20fd5266-3f63-4e49-ae0b-795767667a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# MODEL = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91710dd3-0196-4c7b-812c-b319832c1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "VRAM(GB): 79.3\n",
      "Model max_position_embeddings: 32768\n",
      "INFO 01-10 03:11:39 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 24576, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-32B-Instruct'}\n",
      "INFO 01-10 03:11:40 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 01-10 03:11:40 [model.py:1661] Using max model len 24576\n",
      "INFO 01-10 03:11:41 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-10 03:11:42 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:11:50 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-32B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=24576, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-32B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m WARNING 01-10 03:11:50 [network_utils.py:36] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:11:50 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.200.4.36:43705 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:11:50 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:11:51 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m /home/mlcore/conda/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:11:54 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:09,  1.64it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:01<00:10,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:02<00:09,  1.42it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:02<00:09,  1.38it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:03<00:08,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:04<00:08,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:05<00:07,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:05<00:06,  1.35it/s]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:06<00:05,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:07<00:05,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:08<00:04,  1.33it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:08<00:03,  1.32it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:09<00:03,  1.28it/s]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:10<00:02,  1.29it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:11<00:01,  1.38it/s]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:11<00:00,  1.41it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:12<00:00,  1.40it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:12<00:00,  1.36it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:08 [default_loader.py:308] Loading weights took 12.57 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:09 [gpu_model_runner.py:3659] Model loading took 61.0375 GiB memory and 17.145612 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:21 [backends.py:643] Using cache directory: /home/mlcore/.cache/vllm/torch_compile_cache/956b349265/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:21 [backends.py:703] Dynamo bytecode transform time: 12.13 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:41 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 13.879 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:41 [monitor.py:34] torch.compile takes 26.01 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:43 [gpu_worker.py:375] Available KV cache memory: 8.81 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:43 [kv_cache_utils.py:1291] GPU KV cache size: 36,064 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:43 [kv_cache_utils.py:1296] Maximum concurrency for 24,576 tokens per request: 1.47x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:06<00:00,  7.55it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:03<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:54 [gpu_model_runner.py:4587] Graph capturing finished in 11 secs, took 4.11 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=9069)\u001b[0;0m INFO 01-10 03:12:54 [core.py:259] init engine (profile, create kv cache, warmup model) took 45.44 seconds\n",
      "INFO 01-10 03:12:56 [llm.py:360] Supported tasks: ['generate']\n",
      "ERROR 01-10 03:17:32 [core_client.py:606] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "MODEL = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "#MODEL = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(MODEL, trust_remote_code=True)\n",
    "native_ctx = getattr(cfg, \"max_position_embeddings\", None)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"VRAM(GB):\", round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 1))\n",
    "print(\"Model max_position_embeddings:\", native_ctx)\n",
    "\n",
    "MAX_MODEL_LEN = 24576\n",
    "\n",
    "\n",
    "model = LLM(\n",
    "    model=MODEL,\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=0.90,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaab1396-2f2f-4691-80b6-4ddf925d8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"–¢—ã ‚Äî –æ—á–µ–Ω—å –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –¢—ã –ø–∏—à–µ—à—å –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ –Ω–æ–≤–æ—Å—Ç—è–º –≤ —Å—Ç–∏–ª–µ –∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–∞.\n",
    "\n",
    "–í—Ö–æ–¥: –∑–∞–ø—Ä–æ—Å, –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD) –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤–∏–¥–∞:\n",
    "[id] date=YYYY-MM-DD channel(s)=<–∫–∞–Ω–∞–ª1; –∫–∞–Ω–∞–ª2; ...>\n",
    "<—Ç–µ–∫—Å—Ç>\n",
    "–í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã –ù–ï –ü–û–ó–ñ–ï –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–∞—Ç—ã.\n",
    "\n",
    "–û–ë–©–ò–ï –ü–†–ê–í–ò–õ–ê:\n",
    "1) –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "2) –ù–µ —É–ø–æ–º–∏–Ω–∞–π –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ([id]).\n",
    "3) –ù–µ –¥–æ–±–∞–≤–ª—è–π –¥–∞—Ç, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π ‚Äú—Å–µ–≥–æ–¥–Ω—è/–≤—á–µ—Ä–∞/–Ω–µ–¥–∞–≤–Ω–æ‚Äù ‚Äî —Ç–æ–ª—å–∫–æ YYYY-MM-DD.\n",
    "4) –õ—é–±—ã–µ —á–∏—Å–ª–∞/—Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è/–ø–æ—Ä–æ–≥–∏/—Ü–∏—Ç–∞—Ç—ã/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.\n",
    "5) –ö–∞–Ω–∞–ª—ã —É–∫–∞–∑—ã–≤–∞–π –¢–û–õ–¨–ö–û –≤ –¥–∞–π–¥–∂–µ—Å—Ç–µ (–≤ —Ç–∞–π–º–ª–∞–π–Ω–µ –∫–∞–Ω–∞–ª–æ–≤ –Ω–µ –±—É–¥–µ—Ç).\n",
    "6) –ï—Å–ª–∏ —Ä—è–¥–æ–º —Å –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–∞—Ç–æ–π –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ‚Äî —á–µ—Å—Ç–Ω–æ —É–∫–∞–∂–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –≤ –ø–æ–¥–±–æ—Ä–∫–µ –∏ —Ä–∞–∑—Ä—ã–≤, –±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤ ‚Äú—á—Ç–æ —Å–µ–π—á–∞—Å‚Äù.\n",
    "7) –ù–µ–ª—å–∑—è –≤—Å—Ç–∞–≤–ª—è—Ç—å ‚Äú–ø—É—Å—Ç—ã–µ‚Äù –¥–∞—Ç—ã –∏ —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ ‚Äú–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö‚Äù.\n",
    "\n",
    "–ö–ê–ö –ü–ò–°–ê–¢–¨ –î–ê–ô–î–ñ–ï–°–¢:\n",
    "- –ù–∞—á–Ω–∏ —Å 2‚Äì4 —Å–∞–º—ã—Ö —Å–≤–µ–∂–∏—Ö –£–ù–ò–ö–ê–õ–¨–ù–´–• –¥–∞—Ç –≤ –ø–æ–¥–±–æ—Ä–∫–µ (—ç—Ç–æ ‚Äú–ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è‚Äù).\n",
    "- –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Ñ–∞–∫—Ç–∞ —É–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫:\n",
    "  ‚Äú–ö–∞–Ω–∞–ª(—ã) (YYYY-MM-DD): ‚Ä¶‚Äù\n",
    "- –ù–µ —Å–º–µ—à–∏–≤–∞–π —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —Å–∏–≥–Ω–∞–ª–æ–≤ –∫–∞–∫ –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ: —è–≤–Ω–æ —Ä–∞–∑–ª–∏—á–∞–π\n",
    "  ‚Äú–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å / –≤–Ω–µ–±–∏—Ä–∂–µ–≤–æ–π –∫—É—Ä—Å / —Ä–∞—Å—á–µ—Ç–Ω—ã–π –∫—É—Ä—Å / –∏–Ω–¥–µ–∫—Å –¥–æ–ª–ª–∞—Ä–∞ / –ø—Ä–æ–≥–Ω–æ–∑‚Äù.\n",
    "- –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –ø–æ–≤—Ç–æ—Ä—ã ‚Äî –æ–±—ä–µ–¥–∏–Ω—è–π.\n",
    "- –ü–∏—à–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ, –∫–∞–∫ –≤ –Ω–æ–≤–æ—Å—Ç–Ω–æ–º –¥–∞–π–¥–∂–µ—Å—Ç–µ (–±–µ–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –∏ –±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤).\n",
    "\n",
    "–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê (3 –±–ª–æ–∫–∞):\n",
    "\n",
    "### 1) –ó–∞–ø—Ä–æ—Å –∏ –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞\n",
    "* –ó–∞–ø—Ä–æ—Å: ...\n",
    "* –ê–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞: ...\n",
    "\n",
    "### 2) –î–∞–π–¥–∂–µ—Å—Ç\n",
    "–°–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç (8‚Äì20 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π), –∫–∞–∫ –∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç:\n",
    "- ‚Äú–ü–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è‚Äù: –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –ø–æ —Å–∞–º—ã–º —Å–≤–µ–∂–∏–º –¥–∞—Ç–∞–º + –∫—Ç–æ —Å–æ–æ–±—â–∏–ª.\n",
    "- –ó–∞—Ç–µ–º –∫–æ—Ä–æ—Ç–∫–æ ‚Äú—Ä–∞–Ω—å—à–µ/–ø—Ä–µ–¥—ã—Å—Ç–æ—Ä–∏—è‚Äù: –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –ø–æ –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–º –¥–∞—Ç–∞–º + –∫—Ç–æ —Å–æ–æ–±—â–∏–ª.\n",
    "- –í –∫–æ–Ω—Ü–µ 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –≤—ã–≤–æ–¥—ã, –∏—Å—Ö–æ–¥—è –∏–∑ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–µ–π. –ú–æ–∂–Ω–æ –ª–∏ –¥–∞—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É –∏–ª–∏ –Ω–µ—Ç. –ù–∞—Å–∫–æ–ª—å–∫–æ —Ç–µ–º–∞ –≤ —Ü–µ–ª–æ–º –∞–∫—Ç—É–∞–ª—å–Ω–∞—è, —Å–≤–µ–∂–∞—è –∏ –≤–∏—Ä—É—Å–Ω–∞—è.\n",
    "\n",
    "### 3) –¢–∞–π–º–ª–∞–π–Ω (–ø–æ–ª–Ω—ã–π, 1 –¥–∞—Ç–∞ = 1 —Å—Ç—Ä–æ–∫–∞ = 1 —Ñ–∞–∫—Ç, –±–µ–∑ –∫–∞–Ω–∞–ª–æ–≤)\n",
    "–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –í–°–ï–• –£–ù–ò–ö–ê–õ–¨–ù–´–• –¥–∞—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤ –ø–æ—Ä—è–¥–∫–µ –æ—Ç —Å–∞–º–æ–π —Å—Ç–∞—Ä–æ–π –∫ —Å–∞–º–æ–π –Ω–æ–≤–æ–π (ascending).\n",
    "–û–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–Ω–∞ (1) –¥–∞—Ç–∞.\n",
    "–û–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–∏–Ω (1) —Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å—É —Ñ–∞–∫—Ç/–∏–∑–º–µ–Ω–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä—ã–π —è–≤–Ω–æ –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —ç—Ç–æ–π –¥–∞—Ç—ã.\n",
    "\n",
    "–§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–∫–∏:\n",
    "* YYYY-MM-DD ‚Äî —á—Ç–æ —Å–æ–æ–±—â–∏–ª–∏ / —á—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å (1 —Ñ–∞–∫—Ç)\n",
    "\n",
    "–ü–†–ê–í–ò–õ–ê –¢–ê–ô–ú–õ–ê–ô–ù–ê (–∫—Ä–∏—Ç–∏—á–Ω–æ):\n",
    "- –í —Ç–∞–π–º–ª–∞–π–Ω–µ –ù–ï –£–ö–ê–ó–´–í–ê–ô –∫–∞–Ω–∞–ª—ã –≤–æ–æ–±—â–µ.\n",
    "- –ù–µ –¥–æ–±–∞–≤–ª—è–π –¥–∞—Ç, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö!\n",
    "- –ù–µ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ñ–∞–∫—Ç—ã –º–µ–∂–¥—É –¥–∞—Ç–∞–º–∏ ‚Äú–ø–æ —Å–º—ã—Å–ª—É‚Äù: –≤ —Å—Ç—Ä–æ–∫–µ –¥–∞—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —ç—Ç–æ–π –¥–∞—Ç—ã!\n",
    "- –ù–∏–∫–∞–∫–∏—Ö –ø—Ä–∏—á–∏–Ω –∏ –≤—ã–≤–æ–¥–æ–≤ ‚Äî —Ç–æ–ª—å–∫–æ ‚Äú—á—Ç–æ —Å–æ–æ–±—â–∏–ª–∏‚Äù!\n",
    "- –ï—Å–ª–∏ –Ω–∞ –æ–¥–Ω—É –¥–∞—Ç—É –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–æ–≤, –≤—ã–±–µ—Ä–∏ –æ–¥–∏–Ω —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (–æ—Å—Ç–∞–ª—å–Ω–æ–µ –æ—Å—Ç–∞–≤—å –≤ –¥–∞–π–¥–∂–µ—Å—Ç–µ)!\n",
    "! –ï–°–õ–ò –î–ê–¢–´ –ù–ï–¢ –í–ù–£–¢–†–ò –ö–û–ù–¢–ï–ö–°–¢–ê ‚Äî –ï–Å –ù–ï–õ–¨–ó–Ø –£–ö–ê–ó–´–í–ê–¢–¨. –ï–°–õ–ò –§–ê–ö–¢–ê –ù–ï–¢ –ù–ê –≠–¢–£ –î–ê–¢–£ –í –î–û–ö–£–ú–ï–ù–¢–ê–• ‚Äî –ï–ì–û –ù–ï–õ–¨–ó–Ø –ü–ò–°–ê–¢–¨. !\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def build_rag_context(query: str, cand: pd.DataFrame, anchor_date: str, k_docs: int = 30, snip_chars: int = 850) -> str:\n",
    "    c = cand.head(k_docs).copy()\n",
    "\n",
    "    blocks = []\n",
    "    for i, row in enumerate(c.itertuples(index=False), start=1):\n",
    "        date_day = getattr(row, \"date_day\", getattr(row, \"date\", \"\"))\n",
    "        if isinstance(date_day, pd.Timestamp):\n",
    "            date_day = date_day.strftime(\"%Y-%m-%d\")\n",
    "        date_day = str(date_day)[:10]\n",
    "\n",
    "        channel = getattr(row, \"channel_name\")\n",
    "        text = getattr(row, \"message\", \"\")\n",
    "\n",
    "        blocks.append(f\"[{i}] date={date_day} channel(s)={channel}\\n document=\" + snippet(str(text), snip_chars))\n",
    "\n",
    "    return (\n",
    "        f\"–ê–ö–¢–£–ê–õ–¨–ù–ê–Ø –î–ê–¢–ê –û–ë–ó–û–†–ê: {anchor_date}\\n\"\n",
    "        f\"–í–û–ü–†–û–°/–ó–ê–ü–†–û–°:\\n{query}\\n\\n\"\n",
    "        f\"–ò–°–¢–û–ß–ù–ò–ö–ò:\\n\" + \"\\n\\n\".join(blocks)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0bfef7-03d2-48bf-8bcb-6910245c8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "import pandas as pd\n",
    "from vllm import SamplingParams\n",
    "\n",
    "JUDGE_SYSTEM = \"\"\"–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫—É –ø–æ –Ω–æ–≤–æ—Å—Ç—è–º (–≤ —Ç.—á. —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–º).\n",
    "\n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –æ—Ü–µ–Ω–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å—É. –ó–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å:\n",
    "- –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–æ–ø–∏–∫–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä \"–∫—É—Ä—Å —Ä—É–±–ª—è –∫ –¥–æ–ª–ª–∞—Ä—É\"),\n",
    "- –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–º –¥—Ä—É–≥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ (—Ç–æ–≥–¥–∞ –∑–∞–ø—Ä–æ—Å –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏–Ω—Ñ–æ–ø–æ–≤–æ–¥).\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ù–∏—á–µ–≥–æ –Ω–µ –¥–æ–¥—É–º—ã–≤–∞–π.\n",
    "\n",
    "–®–∫–∞–ª–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏:\n",
    "2 ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç —è–≤–Ω–æ –ø—Ä–æ —Ç–æ –∂–µ —Å–∞–º–æ–µ: –æ—Ç–≤–µ—á–∞–µ—Ç —Ç–æ–ø–∏–∫—É –ò–õ–ò –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–æ—Ç –∂–µ –∏–Ω—Ñ–æ–ø–æ–≤–æ–¥/—Ñ–∞–∫—Ç/—Å–æ–±—ã—Ç–∏–µ, —á—Ç–æ –∏ –∑–∞–ø—Ä–æ—Å.\n",
    "1 ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç —Å–≤—è–∑–∞–Ω –ø–æ —Ç–µ–º–µ/–∫–æ–Ω—Ç–µ–∫—Å—Ç—É, –Ω–æ —ç—Ç–æ –Ω–µ–º–Ω–æ–≥–æ –¥—Ä—É–≥–æ–π –∏–Ω—Ñ–æ–ø–æ–≤–æ–¥, –∏–ª–∏ –ø—Ä–æ —Ç–æ –∂–µ, –Ω–æ –±–µ–∑ –ø—Ä—è–º–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è.\n",
    "0 ‚Äî –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ —Å–æ–≤—Å–µ–º.\n",
    "\n",
    "–ü—Ä–∞–≤–∏–ª–æ —Å—Ç—Ä–æ–≥–æ—Å—Ç–∏:\n",
    "—Å—Ç–∞–≤—å 2 —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–≤—è–∑—å –æ—á–µ–≤–∏–¥–Ω–∞ –ø–æ —Ç–µ–∫—Å—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞; –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å—Ç–∞–≤—å 0 –∏–ª–∏ 1.\n",
    "\n",
    "–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON –∏ –Ω–∏—á–µ–≥–æ –±–æ–ª—å—à–µ:\n",
    "{\"relevance\": 0|1|2}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _parse_relevance(text: str) -> int:\n",
    "    text = text.strip()\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if m:\n",
    "        blob = m.group(0)\n",
    "        try:\n",
    "            obj = json.loads(blob)\n",
    "            val = int(obj.get(\"relevance\", 0))\n",
    "            return val if val in (0, 1, 2) else 0\n",
    "        except Exception:\n",
    "            pass\n",
    "    m2 = re.search(r\"relevance\\\"\\s*:\\s*([012])\", text)\n",
    "    if m2:\n",
    "        return int(m2.group(1))\n",
    "    return 0\n",
    "\n",
    "def judge_filter_candidates(\n",
    "    cand: pd.DataFrame,\n",
    "    query: str,\n",
    "    judge_llm,\n",
    "    judge_tokenizer,\n",
    "    *,\n",
    "    keep_threshold: int = 1,     \n",
    "    doc_max_chars: int = 1200,\n",
    "    batch_size: int = 32,\n",
    "    max_out_tokens: int = 40,\n",
    ") -> pd.DataFrame:\n",
    "    if cand is None or len(cand) == 0:\n",
    "        return cand\n",
    "\n",
    "    text_col = \"message\"\n",
    "    channel_col = \"channel_name\"\n",
    "    date_col = \"date_day\"\n",
    "\n",
    "    prompts = []\n",
    "    for _, row in cand.iterrows():\n",
    "        doc = str(row[text_col])[:doc_max_chars]\n",
    "        ch = str(row[channel_col]) if channel_col else \"\"\n",
    "        dt = str(row[date_col]) if date_col else \"\"\n",
    "\n",
    "        user_msg = (\n",
    "            f\"–ó–ê–ü–†–û–°:\\n{query}\\n\\n\"\n",
    "            f\"–ö–ê–ù–î–ò–î–ê–¢:\\n\"\n",
    "            f\"channel={ch}\\n\"\n",
    "            f\"date={dt}\\n\"\n",
    "            f\"text:\\n{doc}\\n\"\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "        prompt = judge_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    sampling = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=max_out_tokens,\n",
    "    )\n",
    "\n",
    "    relevances = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        outs = judge_llm.generate(batch_prompts, sampling)\n",
    "        for o in outs:\n",
    "            txt = o.outputs[0].text\n",
    "            relevances.append(_parse_relevance(txt))\n",
    "\n",
    "    out_df = cand.copy()\n",
    "    out_df[\"judge_relevance\"] = relevances\n",
    "\n",
    "    filtered = out_df[out_df[\"judge_relevance\"] >= keep_threshold].copy()\n",
    "    filtered.reset_index(drop=True, inplace=True)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b04c688-b520-422b-ab98-275c68d10ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# @torch.inference_mode()\n",
    "# def rag_summarize(sum_model, sum_tokenizer, query: str, cand: pd.DataFrame, anchor_date, \n",
    "#                   k_docs: int = 25, snip_chars: int = 900, max_new_tokens: int = 2000) -> str:\n",
    "    \n",
    "#     user = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "#     print(\"built context...\")\n",
    "    \n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "#         {\"role\": \"user\", \"content\": user},\n",
    "#     ]\n",
    "#     prompt = sum_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     enc = sum_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(sum_model.device)\n",
    "\n",
    "#     out_ids = sum_model.generate(\n",
    "#         **enc,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         do_sample=False,\n",
    "#         eos_token_id=sum_tokenizer.eos_token_id,\n",
    "#         pad_token_id=sum_tokenizer.eos_token_id,\n",
    "#     )\n",
    "#     prompt_len = int(enc[\"attention_mask\"][0].sum().item())\n",
    "    \n",
    "#     return sum_tokenizer.decode(out_ids[0][prompt_len:], skip_special_tokens=True).strip(), user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a6f6c0-27c0-48ec-b32f-94dcc09ce244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "def rag_summarize(sum_model, sum_tokenizer, query: str, cand: pd.DataFrame, anchor_date,\n",
    "                  k_docs: int = 25, snip_chars: int = 900, max_new_tokens: int = 2000):\n",
    "\n",
    "    user = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "    print(\"built context...\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "    prompt = sum_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    #Mistral\n",
    "    #prompt = f\"<s>[SYSTEM_PROMPT]{SYSTEM_PROMPT}[/SYSTEM_PROMPT][INST]{user}[/INST]\"\n",
    "\n",
    "    sampling = SamplingParams(\n",
    "        temperature=0.0,  \n",
    "        max_tokens=max_new_tokens,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "    result = sum_model.generate([prompt], sampling)[0]\n",
    "    text = result.outputs[0].text.strip()\n",
    "\n",
    "    return text, user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "382a7922-4c1b-4d3f-bbd7-95152e4842e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_hybrid(\n",
    "    df: pd.DataFrame,\n",
    "    index,\n",
    "    encoder,\n",
    "    bm25,\n",
    "    tokenize_fn,\n",
    "    query: str,\n",
    "    exclude_message_id: str | None = None,\n",
    "    k_retrieve: int = 50,\n",
    "    topN_each: int = 500,\n",
    "    k_docs: int = 25,\n",
    "    snip_chars: int = 1500,\n",
    "    max_new_tokens: int = 2000,\n",
    "    anchor_date: str = \"2025-09-04\",\n",
    "    \n",
    "    sum_model=None,\n",
    "    sum_tokenizer=None,\n",
    "\n",
    "    judge_llm=None,\n",
    "    judge_tokenizer=None,\n",
    "    judge_keep_threshold: int = 1,\n",
    "    judge_batch_size: int = 32,\n",
    "):\n",
    "    cand = hybrid_retrieve_rrf(\n",
    "        df=df,\n",
    "        index=index,\n",
    "        encoder=encoder,\n",
    "        bm25=bm25,\n",
    "        tokenize_fn=tokenize_fn,\n",
    "        query=query,\n",
    "        k=k_retrieve,\n",
    "        topN_each=topN_each,\n",
    "        k_rrf=60,\n",
    "        w_dense=1.0,\n",
    "        w_bm25=1.0,\n",
    "        exclude_message_id=exclude_message_id,\n",
    "        anchor_date=anchor_date\n",
    "    )\n",
    "\n",
    "    cand_before = cand\n",
    "    if judge_llm is not None and judge_tokenizer is not None and cand is not None and len(cand) > 0:\n",
    "        cand = judge_filter_candidates(\n",
    "            cand=cand,\n",
    "            query=query,\n",
    "            judge_llm=judge_llm,\n",
    "            judge_tokenizer=judge_tokenizer,\n",
    "            keep_threshold=judge_keep_threshold,\n",
    "            doc_max_chars=snip_chars,\n",
    "            batch_size=judge_batch_size,\n",
    "        )\n",
    "\n",
    "    if sum_model is None or sum_tokenizer is None:\n",
    "        ctx = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "        return {\n",
    "            \"context\": ctx,\n",
    "            \"candidates\": cand_before,\n",
    "            \"candidates_filtered\": cand,\n",
    "            \"summary\": \"No LLM\",\n",
    "        }\n",
    "\n",
    "    summary, ctx = rag_summarize(\n",
    "        sum_model, sum_tokenizer,\n",
    "        query, cand,\n",
    "        k_docs=min(k_docs, len(cand)) if cand is not None else 0,\n",
    "        snip_chars=snip_chars,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        anchor_date=anchor_date\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"context\": ctx,\n",
    "        \"summary\": summary,\n",
    "        \"candidates\": cand_before,\n",
    "        \"candidates_filtered\": cand,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453b4b7-8475-4e75-87bf-3fd23950401c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4cdc31c-ad49-4c99-bab8-8d16d73c7f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d32c711e11494c89f6bb9c1add3966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f576c64692f4d3b8b585d72215e2fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956a074461c845dc883284be7f050f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2084824d5df5424d9a2fd409d12fb51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d335c5c37904695a1ce9f056483b769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a23654bf1944f9902642508a3238ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36be269a05154046b461e29112257f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802ae2f9ad8f4c8aa9143efeba227e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7474554381f64679bab318188e0e1117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90948d0a3b944799bf178182e9d466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built context...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c453dd1614d4b1882d66633fdcf6f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ee8dc73a8242159d490a683aecab3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = \"–ê–∫—Ç—É–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞\"\n",
    "\n",
    "out = run_rag_hybrid(\n",
    "    df=df,\n",
    "    index=index,\n",
    "    encoder=encoder,\n",
    "    bm25=bm25,\n",
    "    tokenize_fn=tokenize_ru,\n",
    "    query=q,\n",
    "\n",
    "    k_retrieve=150,\n",
    "    topN_each=2000,\n",
    "    k_docs=30,\n",
    "    snip_chars=1500,\n",
    "    max_new_tokens=5000,\n",
    "    anchor_date=\"2025-09-04\",\n",
    "\n",
    "    sum_model=model,\n",
    "    sum_tokenizer=tokenizer,\n",
    "\n",
    "    judge_llm=model,\n",
    "    judge_tokenizer=tokenizer,\n",
    "    judge_keep_threshold=1,  \n",
    "    judge_batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8f8574b-ad55-46f5-b75f-9c8ef1738f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 1) –ó–∞–ø—Ä–æ—Å –∏ –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞\n",
       "* –ó–∞–ø—Ä–æ—Å: –ê–∫—Ç—É–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞\n",
       "* –ê–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞: 2025-09-04\n",
       "\n",
       "### 2) –î–∞–π–¥–∂–µ—Å—Ç\n",
       "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è, —Å –∏—é–ª—è –ø–æ –∏—é–Ω—å 2025 –≥–æ–¥–∞, –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–ª —Å–Ω–∏–∂–∞—Ç—å—Å—è. 28 –∏—é–ª—è 2025 –≥–æ–¥–∞, —Å–æ–≥–ª–∞—Å–Ω–æ –¥–∞–Ω–Ω—ã–º –¶–ë, –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ –≤–Ω–µ–±–∏—Ä–∂–µ–≤–æ–≥–æ –Ω–∞ –ø–æ–ª—Ç–æ—Ä–∞ —Ä—É–±–ª—è. 29 –º–∞—è 2025 –≥–æ–¥–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ—Ç –¶–ë –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 79 —Ä—É–±–ª–µ–π, —Å–æ—Å—Ç–∞–≤–∏–≤ 78,5 —Ä—É–±–ª—è. 30 –∏—é–Ω—è 2025 –≥–æ–¥–∞ –∏–Ω–¥–µ–∫—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 97 –ø—É–Ω–∫—Ç–æ–≤ –≤–ø–µ—Ä–≤—ã–µ —Å –º–∞—Ä—Ç–∞ 2022 –≥–æ–¥–∞. –†–∞–Ω–µ–µ, 19 –º–∞—Ä—Ç–∞ 2025 –≥–æ–¥–∞, –≤–Ω–µ–±–∏—Ä–∂–µ–≤–æ–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 81 —Ä—É–±–ª—è. \n",
       "\n",
       "–†–∞–Ω—å—à–µ, –≤ –∞–ø—Ä–µ–ª–µ 2025 –≥–æ–¥–∞, –¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∏–∂–µ 81 —Ä—É–±–ª—è –≤–ø–µ—Ä–≤—ã–µ —Å –ª–µ—Ç–∞ 2023 –≥–æ–¥–∞. –í –º–∞—Ä—Ç–µ 2025 –≥–æ–¥–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –Ω–∏–∂–µ 83 —Ä—É–±–ª–µ–π, –≤–ø–µ—Ä–≤—ã–µ —Å –ø—Ä–æ—à–ª–æ–≥–æ –ª–µ—Ç–∞. –í —Ñ–µ–≤—Ä–∞–ª–µ 2025 –≥–æ–¥–∞ –¶–ë —Ç–∞–∫–∂–µ –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 86 —Ä—É–±–ª–µ–π.\n",
       "\n",
       "–í —Ü–µ–ª–æ–º, —Ç–µ–º–∞ –∫—É—Ä—Å–∞ –¥–æ–ª–ª–∞—Ä–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏ –≤–∏—Ä—É—Å–Ω–æ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–≤–µ—Ç–µ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–æ–ª–µ–±–∞–Ω–∏–π –∏ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –¶–ë. –û–¥–Ω–∞–∫–æ, –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∫—É—Ä—Å—É –¥–æ–ª–ª–∞—Ä–∞ –¥–∞—Ç–∏—Ä—É—é—Ç—Å—è –∏—é–Ω–µ–º 2025 –≥–æ–¥–∞, —á—Ç–æ –æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑—Ä—ã–≤ –¥–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–∞—Ç—ã –æ–±–∑–æ—Ä–∞.\n",
       "\n",
       "### 3) –¢–∞–π–º–ª–∞–π–Ω\n",
       "* 2024-11-14 ‚Äî –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –≤—ã—à–µ 100 —Ä—É–±–ª–µ–π\n",
       "* 2024-11-27 ‚Äî –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –ø–µ—Ä–µ–≤–∞–ª–∏–ª –∑–∞ 111 —Ä—É–±–ª–µ–π, –∑–∞—Ç–µ–º –¥–æ 114 —Ä—É–±–ª–µ–π\n",
       "* 2024-11-28 ‚Äî –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 110 —Ä—É–±–ª–µ–π\n",
       "* 2024-12-05 ‚Äî –¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –Ω–æ–≤—ã–µ –∫—É—Ä—Å—ã –¥–ª—è –¥–æ–ª–ª–∞—Ä–∞ –∏ –µ–≤—Ä–æ ‚Äî 103,3 –∏ 109,7 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ\n",
       "* 2024-12-06 ‚Äî –¶–ë –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∏–∂–µ 100, –µ–≤—Ä–æ ‚Äî –Ω–∏–∂–µ 107\n",
       "* 2024-12-24 ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –Ω–∏–∂–µ 100\n",
       "* 2025-02-25 ‚Äî –¶–ë –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 86 —Ä—É–±–ª–µ–π\n",
       "* 2025-03-11 ‚Äî –¶–ë –æ–ø—É—Å—Ç–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 86 —Ä—É–±–ª–µ–π\n",
       "* 2025-03-17 ‚Äî —Ä–∞—Å—á–µ—Ç–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –Ω–∏–∂–µ 83 —Ä—É–±–ª–µ–π\n",
       "* 2025-03-18 ‚Äî –≤–Ω–µ–±–∏—Ä–∂–µ–≤–æ–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 81 —Ä—É–±–ª—è\n",
       "* 2025-04-01 ‚Äî –¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∏–∂–µ 85 —Ä—É–±–ª–µ–π\n",
       "* 2025-04-14 ‚Äî –¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫ –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 1 —Ä—É–±–ª—å\n",
       "* 2025-04-21 ‚Äî –¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∏–∂–µ 81 —Ä—É–±–ª—è –≤–ø–µ—Ä–≤—ã–µ —Å –ª–µ—Ç–∞ 2023 –≥–æ–¥–∞\n",
       "* 2025-05-21 ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ 79,75 —Ä—É–±–ª—è\n",
       "* 2025-05-29 ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 79 —Ä—É–±–ª–µ–π\n",
       "* 2025-06-20 ‚Äî —Ä–∞–≤–Ω–æ–≤–µ—Å–Ω—ã–º –∫—É—Ä—Å–æ–º —Å—á–∏—Ç–∞–µ—Ç—Å—è –≤—ã—à–µ 100 —Ä—É–±–ª–µ–π –∑–∞ –¥–æ–ª–ª–∞—Ä\n",
       "* 2025-06-30 ‚Äî –∏–Ω–¥–µ–∫—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 97 –ø—É–Ω–∫—Ç–æ–≤ –≤–ø–µ—Ä–≤—ã–µ —Å –º–∞—Ä—Ç–∞ 2022 –≥–æ–¥–∞\n",
       "* 2025-07-28 ‚Äî –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ—Ç –¶–ë –æ–∫–∞–∑–∞–ª—Å—è –º–µ–Ω—å—à–µ –≤–Ω–µ–±–∏—Ä–∂–µ–≤–æ–≥–æ –Ω–∞ –ø–æ–ª—Ç–æ—Ä–∞ —Ä—É–±–ª—è"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "import re\n",
    "\n",
    "def show_summary(summary: str):\n",
    "    if summary is None:\n",
    "        display(HTML(\"<b>summary is None</b>\"))\n",
    "        return\n",
    "\n",
    "    s = str(summary)\n",
    "    s = s.replace(\"\\\\n\", \"\\n\") \n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n",
    "\n",
    "    display(Markdown(s))\n",
    "\n",
    "show_summary(out[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4e25602-c922-4041-abc4-2fd0f98f6c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ê–ö–¢–£–ê–õ–¨–ù–ê–Ø –î–ê–¢–ê –û–ë–ó–û–†–ê: 2025-09-04\\n–í–û–ü–†–û–°/–ó–ê–ü–†–û–°:\\n–ê–∫—Ç—É–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞\\n\\n–ò–°–¢–û–ß–ù–ò–ö–ò:\\n[1] date=2024-11-27 channel(s)=–ë–ª—É–º–±–µ—Ä–≥\\n document=–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –ø–µ—Ä–µ–≤–∞–ª–∏–ª –∑–∞ 111 —Ä—É–±–ª–µ–π. UPD: –ö—É—Ä—Å —É–∂–µ 114. üîµ Bloomberg\\n\\n[2] date=2025-07-28 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–°–≤–µ–∂–∏–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ—Ç –¶–ë –æ–∫–∞–∑–∞–ª—Å—è –º–µ–Ω—å—à–µ –≤–Ω–µ–±–∏—Ä–∂–µ–≤–æ–≥–æ –Ω–∞ –ø–æ–ª—Ç–æ—Ä–∞ —Ä—É–±–ª—è. @bankrollo\\n\\n[3] date=2024-12-06 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–¶–ë –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∏–∂–µ 100, –µ–≤—Ä–æ ‚Äî –Ω–∏–∂–µ 107. @bankrollo\\n\\n[4] date=2025-04-21 channel(s)=–°–∏–≥–Ω–∞–ª—ã –†–¶–ë\\n document=–¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∏–∂–µ 81 —Ä—É–±–ª—è –≤–ø–µ—Ä–≤—ã–µ —Å –ª–µ—Ç–∞ 2023 –≥–æ–¥–∞ –¶–ë –†–§ —Å 22 –∞–ø—Ä–µ–ª—è —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –°–®–ê –Ω–∞ 37.74 –∫–æ–ø., –¥–æ 80.7597 —Ä—É–±.\\n\\n[5] date=2024-11-14 channel(s)=–≠–∫–æ–Ω–æ–º–∏–∫–∞ \\n document=–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —Ç–µ–ø–µ—Ä—å —Å—Ç–æ–∏—Ç –≤—ã—à–µ 100 —Ä—É–±–ª–µ–π. –≠—Ç–æ —Å–ª–µ–¥—É–µ—Ç –∏–∑ –¥–∞–Ω–Ω—ã—Ö –ú–æ—Å–±–∏—Ä–∂–∏. ¬´–°—Ç–æ–∏–º–æ—Å—Ç—å –¥–æ–ª–ª–∞—Ä–∞ ‚Äî 100,2428 —Ä—É–±–ª—è¬ª, ‚Äî —Å–æ–æ–±—â–∞–µ—Ç —Ä–µ–≥—É–ª—è—Ç–æ—Ä. –¢–∞–∫–æ–≥–æ —Å–∫–∞—á–∫–∞ –Ω–µ –±—ã–ª–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Å—è—Ü–µ–≤. –°–µ–π—á–∞—Å –¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫ —Å–∞–º —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —Ü–µ–Ω—É –¥–æ–ª–ª–∞—Ä–∞ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ —Ä—É–±–ª—é. –≠—Ç–æ —Å—Ç–∞–ª–æ —Å–ª–µ–¥—Å—Ç–≤–∏–µ–º —Å–∞–Ω–∫—Ü–∏–π –°–®–ê. ùîº‚ÑÇùïÜ‚ÑïùïÜùïÑùïÄùïÇùî∏\\n\\n[6] date=2025-05-29 channel(s)=–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏\\n document=–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ—Ç –¶–ë –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 79 —Ä—É–±–ª–µ–π. –ï–≥–æ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ —á–µ—Ç–≤–µ—Ä–≥ - 78,5 —Ä—É–±–ª—è.\\n\\n[7] date=2025-06-30 channel(s)=–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏\\n document=–ò–Ω–¥–µ–∫—Å –¥–æ–ª–ª–∞—Ä–∞, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π –∫—É—Ä—Å –∫ –≤–∞–ª—é—Ç–∞–º —à–µ—Å—Ç–∏ —Å—Ç—Ä–∞–Ω - —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ –°–®–ê, —Å–µ–≥–æ–¥–Ω—è –æ–ø—É—Å–∫–∞–ª—Å—è –Ω–∏–∂–µ 97 –ø—É–Ω–∫—Ç–æ–≤ –≤–ø–µ—Ä–≤—ã–µ —Å –º–∞—Ä—Ç–∞ 2022-–≥–æ –ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏ / –í—Å–µ –Ω–∞—à–∏ –∫–∞–Ω–∞–ª—ã\\n\\n[8] date=2025-03-17 channel(s)=–†–∞–Ω—å—à–µ –≤—Å–µ—Ö. –ù—É –ø–æ—á—Ç–∏\\n document=–†–∞—Å—á–µ—Ç–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ - –æ—Ä–∏–µ–Ω—Ç–∏—Ä –¥–ª—è –≤–Ω–µ–±–∏—Ä–∂–µ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞ - —É–ø–∞–ª –Ω–∏–∂–µ 83 —Ä—É–±–ª–µ–π, –≤–ø–µ—Ä–≤—ã–µ —Å –ø—Ä–æ—à–ª–æ–≥–æ –ª–µ—Ç–∞, —Å–ª–µ–¥—É–µ—Ç –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤. –ö 17.02 –º—Å–∫ –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –ø–∞–¥–∞–µ—Ç –Ω–∞ 2,52 —Ä—É–±–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–∫—Ä—ã—Ç–∏—è - –¥–æ 82,87 —Ä—É–±–ª—è, –º–∏–Ω–∏–º—É–º–∞ —Å 27 –∞–≤–≥—É—Å—Ç–∞ –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞.\\n\\n[9] date=2024-04-19 channel(s)=–ë–ª—É–º–±–µ—Ä–≥\\n document=–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å 150-160 —Ä—É–±–ª–µ–π. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–∫–æ–Ω–æ–º–∏—Å—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç, —á—Ç–æ –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –∫ –∏—é–Ω—é –ø—Ä–µ–≤—ã—Å–∏—Ç 100 —Ä—É–±–ª–µ–π –∏ –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å 120 —Ä—É–±–ª–µ–π. –ü–æ –º–Ω–µ–Ω–∏—é —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –≤—ã—Å–æ–∫–∏–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±—É–¥–µ—Ç —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—é —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–æ—Å—Ç–∞ –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –∫ –∫–æ–Ω—Ü—É –≥–æ–¥–∞ –¥–æ–ª–∂–µ–Ω —Å–æ—Å—Ç–∞–≤–∏—Ç—å 150-160 —Ä—É–±–ª–µ–π. üí∏ BloomEconomy\\n\\n[10] date=2025-04-14 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫ –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 1 —Ä—É–±–ª—å, –µ–≤—Ä–æ ‚Äî –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 2. @bankrollo\\n\\n[11] date=2024-04-30 channel(s)=–ë–ª—É–º–±–µ—Ä–≥\\n document=–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –∫—É—Ä—Å–∞ –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –Ω–∏–∂–µ ‚ÇΩ93. –ü—Ä–µ–≤—ã—Å–∏–≤ –Ω–∞–∫–∞–Ω—É–Ω–µ –æ—Ç–º–µ—Ç–∫—É ‚ÇΩ93, –¥–æ–ª–ª–∞—Ä –ø–æ–∫–∞ –Ω–µ –º–æ–∂–µ—Ç –∑–∞–∫—Ä–µ–ø–∏—Ç—å—Å—è –≤—ã—à–µ –Ω–µ–µ. –ï–≤—Ä–æ —Ç–æ—Ä–≥—É–µ—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ ‚ÇΩ99,8, —Ö–æ—Ç—è —Å–µ–≥–æ–¥–Ω—è –≤ —Ä–∞–π–æ–Ω–µ –ø–æ–ª—É–¥–Ω—è –ø—Ä–æ–±–∏–≤–∞–ª —É—Ä–æ–≤–µ–Ω—å ‚ÇΩ100. üîµ BloomEconomy\\n\\n[12] date=2025-03-11 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–¶–ë –æ–ø—É—Å—Ç–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 86 —Ä—É–±–ª–µ–π. –ï–≤—Ä–æ —É–ø–∞–ª –¥–æ 93,6 —Ä—É–±–ª—è. @bankrollo\\n\\n[13] date=2024-06-13 channel(s)=–ë–ª—É–º–±–µ—Ä–≥\\n document=–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –¥–æ 87‚ÇΩ. üîµ Bloomberg\\n\\n[14] date=2025-06-19 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –≤ —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –≤ —Ä–∞–π–æ–Ω–µ 100 —Ä—É–±–ª–µ–π –∑–∞ –¥–æ–ª–ª–∞—Ä, –∑–∞—è–≤–∏–ª –ø–µ—Ä–≤—ã–π –≤–∏—Ü–µ-–ø—Ä–µ–º—å–µ—Ä –ú–∞–Ω—Ç—É—Ä–æ–≤. –°–µ–≥–æ–¥–Ω—è –ø—Ä–∏ –∫—Ä–µ–ø–∫–æ–º —Ä—É–±–ª–µ —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –º–∞–ª–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º, –æ—Ç–º–µ—Ç–∏–ª –æ–Ω. @bankrollo\\n\\n[15] date=2024-12-24 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –Ω–∏–∂–µ 100. @bankrollo\\n\\n[16] date=2024-12-05 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –Ω–æ–≤—ã–µ –∫—É—Ä—Å—ã –¥–ª—è –¥–æ–ª–ª–∞—Ä–∞ –∏ –µ–≤—Ä–æ ‚Äî 103,3 –∏ 109,7 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. @bankrollo\\n\\n[17] date=2024-11-19 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –ø—Ä–µ–≤—ã—Å–∏–ª 100 —Ä—É–±–ª–µ–π. @bankrollo\\n\\n[18] date=2025-03-18 channel(s)=–†–∞–Ω—å—à–µ –≤—Å–µ—Ö. –ù—É –ø–æ—á—Ç–∏\\n document=–í–Ω–µ–±–∏—Ä–∂–µ–≤–æ–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 81 —Ä—É–±–ª—è\\n\\n[19] date=2025-06-30 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–ò–Ω–¥–µ–∫—Å –¥–æ–ª–ª–∞—Ä–∞ —Ä—É—Ö–Ω—É–ª –Ω–∏–∂–µ 97 –ø—É–Ω–∫—Ç–æ–≤ –≤–ø–µ—Ä–≤—ã–µ —Å –º–∞—Ä—Ç–∞ 2022 –≥–æ–¥–∞. –û–Ω –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π –≤–∞–ª—é—Ç—ã –∫ –µ–≤—Ä–æ, –∏–µ–Ω–µ, —à–≤–µ–π—Ü–∞—Ä—Å–∫–æ–º—É —Ñ—Ä–∞–Ω–∫—É, –±—Ä–∏—Ç–∞–Ω—Å–∫–æ–º—É —Ñ—É–Ω—Ç—É, –∫–∞–Ω–∞–¥—Å–∫–æ–º—É –¥–æ–ª–ª–∞—Ä—É –∏ —à–≤–µ–¥—Å–∫–æ–π –∫—Ä–æ–Ω–µ. @bankrollo\\n\\n[20] date=2024-04-26 channel(s)=–ë–ª—É–º–±–µ—Ä–≥\\n document=–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫—É—Ä—Å—ã –≤–∞–ª—é—Ç: üá∏ –î–æ–ª–ª–∞—Ä ‚Äì 87.87‚ÇΩ üá™ –ï–≤—Ä–æ ‚Äì 94.1‚ÇΩ üáπ –õ–∏—Ä–∞ ‚Äì 2.66‚ÇΩ üáµüá± –ó–ª–æ—Ç—ã–π ‚Äì 21.91‚ÇΩ –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞: Bitcoin ‚Äì 61172.00$ Ethereum ‚Äì 3371.35$ USDT ‚Äì 1.00$ Ton ‚Äì 7.60$ BloomEconomy\\n\\n[21] date=2025-04-01 channel(s)=–ë–ª—É–º–±–µ—Ä–≥\\n document=–¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∏–∂–µ 85 —Ä—É–±–ª–µ–π. üîµ Bloomberg\\n\\n[22] date=2025-06-20 channel(s)=–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏\\n document=–†–∞–≤–Ω–æ–≤–µ—Å–Ω—ã–º —Å–µ–π—á–∞—Å —è–≤–ª—è–µ—Ç—Å—è –≤–∞–ª—é—Ç–Ω—ã–π –∫—É—Ä—Å –≤—ã—à–µ 100 —Ä—É–±–ª–µ–π –∑–∞ –¥–æ–ª–ª–∞—Ä, –∑–∞—è–≤–∏–ª –≥–ª–∞–≤–∞ –°–±–µ—Ä–±–∞–Ω–∫–∞ –ì–µ—Ä–º–∞–Ω –ì—Ä–µ—Ñ. –û–Ω –¥–æ–±–∞–≤–∏–ª, —á—Ç–æ –Ω—ã–Ω–µ—à–Ω–∏–π ‚Äì –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–∏–π ‚Äì –∫—É—Ä—Å –æ—á–µ–Ω—å –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤—Å–µ —ç–∫—Å–ø–æ—Ä—Ç–Ω—ã–µ –æ—Ç—Ä–∞—Å–ª–∏ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏ –∏ –±—é–¥–∂–µ—Ç. –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ –ø—è—Ç–Ω–∏—Ü—É —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 78,5 —Ä—É–±–ª—è.\\n\\n[23] date=2024-11-27 channel(s)=–≠–∫–æ–Ω–æ–º–∏–∫–∞ \\n document=–ú–∏–Ω–∏—Å—Ç—Ä—ã –∏ –¥–µ–ø—É—Ç–∞—Ç—ã –ø–µ—Ä–µ—Å—Ç–∞–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∫—É—Ä—Å —Ä—É–±–ª—è. –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —á–∏–Ω–æ–≤–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—Å–∫–∞–∑–∞–ª—Å—è –æ —Å–∏—Ç—É–∞—Ü–∏–∏ ‚Äî –≥–ª–∞–≤–∞ –ú–∏–Ω—Ñ–∏–Ω–∞ –ê–Ω—Ç–æ–Ω –°–∏–ª—É–∞–Ω–æ–≤. –û–Ω –Ω–∞–∑–≤–∞–ª —Ç–µ–∫—É—â–∏–π –∫—É—Ä—Å —Ä—É–±–ª—è –±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω—ã–º –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–æ–≤. –°–µ–π—á–∞—Å —Ü–µ–Ω–∞ –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ –±–∏—Ä–∂–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 111 —Ä—É–±–ª–µ–π –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–∞—Å—Ç–∏, –∞ –∫—É—Ä—Å –µ–≤—Ä–æ –≤–∑–ª–µ—Ç–µ–ª –¥–æ 117 —Ä—É–±–ª–µ–π. –° –Ω–∞—á–∞–ª–∞ –Ω–µ–¥–µ–ª–∏ –¥–æ–ª–ª–∞—Ä –≤—ã—Ä–æ—Å –ø–æ—á—Ç–∏ –Ω–∞ 5,4%, –∞ —Å –Ω–∞—á–∞–ª–∞ –Ω–æ—è–±—Ä—è ‚Äî –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 12,5%. ü§ë The –≠–∫–æ–Ω–æ–º–∏—Å—Ç\\n\\n[24] date=2025-05-29 channel(s)=–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏\\n document=–í–Ω–µ–±–∏—Ä–∂–µ–≤–æ–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ 79 —Ä—É–±–ª–µ–π\\n\\n[25] date=2025-05-21 channel(s)=Forbes Russia\\n document=–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª –Ω–∞ 22 –º–∞—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ 79,75 —Ä—É–±–ª—è. –ù–∏–∂–µ 80 —Ä—É–±–ª–µ–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –≤–ø–µ—Ä–≤—ã–µ —Å –º–∞—è 2023 –≥–æ–¥–∞. –ü—Ä–∏ —ç—Ç–æ–º –µ–≤—Ä–æ –ø–æ–¥–æ—Ä–æ–∂–∞–ª –¥–æ 91,29 —Ä—É–±–ª—è, –∞ —é–∞–Ω—å –ø–æ—á—Ç–∏ –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è\\n\\n[26] date=2024-11-22 channel(s)=Forbes Russia\\n document=–¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –≤—ã—à–µ 102 —Ä—É–±–ª–µ–π –≤–ø–µ—Ä–≤—ã–µ —Å –º–∞—Ä—Ç–∞ 2022 –≥–æ–¥–∞. –ù–∞ 23 –Ω–æ—è–±—Ä—è –∫—É—Ä—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –Ω–∞ —É—Ä–æ–≤–Ω–µ 102,58 —Ä—É–±–ª—è. –°–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±—ã–ª –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω 11 –º–∞—Ä—Ç–∞ 2022-–≥–æ, –∫–æ–≥–¥–∞ –Ω–∞ –æ–¥–∏–Ω –¥–µ–Ω—å –æ–Ω –ø—Ä–µ–≤—ã—Å–∏–ª 120 —Ä—É–±–ª–µ–π\\n\\n[27] date=2024-11-27 channel(s)=–°–∏–≥–Ω–∞–ª—ã –†–¶–ë\\n document=–î–æ–ª–ª–∞—Ä\\n\\n[28] date=2025-04-21 channel(s)=–ë–ª—É–º–±–µ—Ä–≥\\n document=–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –¥–æ 79 —Ä—É–±–ª–µ–π. üîµ Bloomberg\\n\\n[29] date=2025-02-25 channel(s)=–ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞\\n document=–¶–ë –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 86 —Ä—É–±–ª–µ–π. @bankrollo\\n\\n[30] date=2024-11-28 channel(s)=–ë–ª—É–º–±–µ—Ä–≥\\n document=–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ –æ—Ç–º–µ—Ç–∫–∏ 110 —Ä—É–±–ª–µ–π. üîµ Bloomberg'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef50f0-d15a-4e45-a5da-2f723be7dc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae4e7d-5f88-4561-afcb-e55ab536c30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bb82f-7f53-42f3-b16c-05f05dc42352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d1d99-c8a5-4c41-8e12-70ccef824d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ebafb-e7ca-423b-949b-86bad48b8378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58069a8-6326-4db4-a4ad-d93f352638de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db71e0-e65e-4243-bbc9-40a57ee5b077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa86a20-14cf-48d4-89e4-80f66537337e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e1578-a774-4a1f-ba07-c3e2db786e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6a85c-9116-434c-84d9-1bf8fc18ebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34296b50-956e-4686-bf34-1e18d2893733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "local/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
