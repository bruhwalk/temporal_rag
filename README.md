# Как работает Temporal RAG

Temporal RAG — это Retrieval-Augmented Generation пайплайн для новостного архива, где **время** — используется вместе с семантикой и лексикой на этапе кандидато-генерации. Пайплайн отвечает на вопрос “что известно по теме на дату X” он:
1) не использует будущие документы относительно `anchor_date`,
2) предпочитает свежие документы рядом с `anchor_date`,
3) при этом сохраняет историю инфоповода (older context),
4) чистит дубли и типовые репосты,
5) дополнительно фильтрует релевантность через LLM-judge,
6) затем суммаризирует с контролем того, какие документы попали в контекст.

Ниже — подробная схема всех этапов.

---

## Входы и выходы пайплайна

### Входы
- `query`: запрос (может быть “топиком” вроде *курс доллара* или “текстом инфоповода” вроде *ЦБ поднял курс доллара выше 100 рублей*).
- `anchor_date`: опорная дата (YYYY-MM-DD), “на какой момент времени мы хотим обзор”.
- `df`: таблица документов (новостей) с текстом и датой.

### Выходы
Пайплайн возвращает:
- `candidates`: топ кандидатов retrieval (dense+bm25+RRF+time),
- `candidates_dedup`: (опционально) после дедупа/кластеров,
- `candidates_filtered`: (опционально) после LLM-judge,
- `clusters` / `members`: служебная информация о кластерах,
- `context`: сформированный контекст под LLM,
- `summary`: финальное саммари (если подключен суммаризатор).

---

## Общая схема

**IndexBuilder (offline / один раз)**  
1) BM25 по токенам  
2) Dense embeddings (E5_large) для всех документов  
3) FAISS индекс по embeddings  

**Pipeline (online / на каждый запрос)**  
1) Retrieval: dense + BM25 → RRF (fusion)  
2) Time-aware бонус: свежесть относительно `anchor_date`  
3) Бонус по каналам (channel_w)  
4) Dedup + clustering: убрать копии и объединить репосты в рамках +- 1 дня
5) LLM-judge: фильтрация на релевантность запросу документов 
6) Сборка контекста: окно актуальных новостей + предыстория  
7) Summarize: генерация ответа по выбранным документам  

---

## 1) Индексация: что готовится заранее

Это `TemporalRAGIndexBuilder`.

### 1.1. BM25 (лексический поиск)
- Каждый документ токенизируется (русская токенизация, очистка, lower-case).
- Строится `BM25Okapi(corpus_tok)` — это быстрый baseline для совпадений по словам.

BM25 хорошо ловит:
- точные совпадения имен, терминов, тикеров,
- редкие слова,
- формулировки, где плотное совпадение токенов важнее семантики.

### 1.2. Dense embeddings (семантический поиск)
- Используется `SentenceTransformer` (E5 large эмбеддинги).
- Документы кодируются в стиле E5:  
  **`"passage: " + text`**
- Embeddings нормализуются, чтобы inner product в FAISS был близок к cosine similarity.

Dense retrieval хорошо ловит:
- переформулировки,
- близкие смыслы,
- слабые лексические пересечения.

### 1.3. FAISS индекс
- `IndexFlatIP(dim)` (inner product).
- Добавляются все `E_docs`.

Зачем: быстро искать top-N по семантике в большом архиве.

---

## 2) Retrieval: Dense + BM25 + RRF (fusion)

Основная retrieval-функция: `hybrid_retrieve_rrf(...)`.

### 2.1. Временной фильтр (temporal masking)
Если задан `anchor_date`, вводится маска допустимых документов:
- **разрешены только документы с датой `<= anchor_date`**
- дополнительно можно ограничить глубину истории: `max_window_days`  
  (например, брать только последние 365 дней до `anchor_date`)

Это ключевое отличие от “обычного RAG”: мы моделируем реальную “информационную картину” на дату X.

### 2.2. Dense retrieval (FAISS)
- Запрос кодируется как:  
  **`"query: " + query`**
- Из FAISS берется `topN_each` документов.
- Каждому документу присваивается **rank_dense** (1-based отсчет).

### 2.3. BM25 retrieval
- Считаются BM25-скоры по всем документам.
- Берутся top `topN_each`.
- Присваивается **rank_bm25**.

### 2.4. Fusion через RRF
Дальше делается устойчивое объединение двух ранжирований — Reciprocal Rank Fusion:

Для каждого документа из объединенного множества:
```
score_rrf =
    w_dense / (k_rrf + rank_dense) +
    w_bm25  / (k_rrf + rank_bm25)
```

Где:
- `k_rrf` сглаживает влияние первых мест,
- `w_dense`, `w_bm25` — веса семантики и лексики.

RRF:
- не требует нормировки разных шкал (cosine vs BM25),
- устойчив к шуму,
- хорошо работает на “тематических” запросах.

---

## 3) Time bonus: приоритет свежих новостей

Это главный temporal-компонент retrieval.

Даже после фильтра `<= anchor_date`, в архиве может быть много релевантного “старья”.
Чтобы поднять свежие документы вокруг `anchor_date`, добавляется “ранг свежести”:

### 3.1. Возраст документа
```
age_days = (anchor_date - doc_date).days
```
(будущие документы уже выкинуты фильтром)

### 3.2. Ранг свежести
- Сортируем кандидатов по `age_days` (меньше = свежее).
- Получаем `rank_time`, где 1 — самый свежий документ.

### 3.3. Добавка к скору
```
score_rrf += w_time / (k_rrf + rank_time)
```

Свойства:
- бонус зависит не от “сколько дней”, а от **ранга** (устойчивость),
- свежие документы получают систематическое преимущество,
- но старые не исчезают полностью (важно для “истории события”).

---

## 4) (Опционально) Channel boost

Если в данных есть вес источника (на основе кол-ва подписчиков), можно добавить небольшой сдвиг:
```
score += w_channel * channel_w
```

Это полезно, если нужно чуть сильнее доверять более популярным каналам.

Если `w_channel` не задан, он оценивается автоматически как небольшой коэффициент порядка 10% от разброса `score_rrf` — то есть “не ломает” ранжирование, а мягко подталкивает.

---

## 5) Dedup + clustering: убрать копии и объединить репосты

Функция: `dedup_cluster_candidates_time(...)`.

Новостные базы часто содержат:
- репосты одного текста,
- слегка перефразированные копии,
- одно событие, но в 10 каналах.

Цель: **свести это к “инфоповодам”**, чтобы:
- не тратить контекст LLM на одно и то же,
- показывать разнообразие источников,
- повысить плотность фактов.

### 5.1. Нормализация для дедупа
Текст приводится к “устойчивому виду”:
- вырезаются URL,
- вырезаются @handles,
- чистятся символы

### 5.2. Быстрый дедуп по (text_hash + date)
- строится хэш нормализованного текста,
- добавляется дата дня (чтобы одинаковые тексты в разные дни не склеить бездумно),
- внутри каждой группы выбирается лучший представитель (обычно по `score_rrf`).

Это дешево и быстро убирает точные/почти точные копии.

### 5.3. Семантическое объединение в кластеры (near-duplicates)
Дальше берутся представители и строится KNN по embeddings:
- similarity считается по cosine (через FAISS IP на нормализованных векторах),
- объединяем пару документов в один кластер, если:
  - `sim >= sim_threshold` (например 0.95),
  - и `|day_diff| <= max_day_diff` (например 1 день).

Ограничение по дням важно: одинаковая тема через месяц — это уже может быть “новая волна”.

### 5.4. Выбор финальных представителей
Из каждого кластера оставляют `keep_per_cluster` (обычно 1) — самый сильный по скору.
Дополнительно:
- считается `cluster_size`,
- формируется `channel_all`: список каналов, где был этот инфоповод,
- опционально перезаписывается `channel_name`, чтобы в итоговом контексте было видно мульти-источниковость.

---

## 6) LLM Judge: строгая проверка релевантности

Функция: `judge_filter_candidates(...)`.

Даже хороший retrieval может возвращать:
- документы “вокруг темы” (курс, экономика),
- но не тот инфоповод (не про конкретное событие).

Judge решает это как классификацию:
- на вход LLM: **query + один документ**,
- на выход: `relevance` по шкале 0/1/2.

### 6.1. Шкала
- `2`: точно тот же инфоповод / факт
- `1`: тематически близко, но не полное совпадение
- `0`: нерелевантно

### 6.2. Фильтрация
Сохраняются документы с `relevance >= keep_threshold` (1).

Практический смысл:
- сильно снижает “шум” перед суммаризацией,
- удерживает в контексте именно то, что надо отвечать,
- особенно полезно для запросов “текстом новости”.

---

## 7) Сборка контекста: “горячее окно” + предыстория

Функция: `build_rag_context(...)`.

Даже после judge у нас может быть много документов. Но LLM-контекст ограничен.
Поэтому выбирается подмножество документов в стратегию:

### 7.1. Горячее окно
- считаем `age_days` относительно `anchor_date`,
- отмечаем документы в диапазоне `[0 .. hot_window_days]` (например 30 дней),
- забираем `hot_ratio` (например 0.8) от `k_docs` именно из “горячих”.

### 7.2. Предыстория
Оставшуюся часть `k_docs` добираем из более старых документов:
- чтобы дать LLM контекст “как это развивалось”,
- и закрыть кейсы, где рядом с anchor_date мало новостей.

### 7.3. Порядок в контексте
Документы сортируются:
- по дате (новые сверху),
- при равной дате — по скору.

Формат каждого блока:
- `[i] date=... channel(s)=...`
- `document=<snippet>`

---

## 8) Summarize: генерация ответа LLM

Функция: `rag_summarize(...)`.

LLM получает:
- `system_prompt` (стиль/структура ответа),
- `user` контекст, собранный на предыдущем шаге.

Важные свойства:
- генерация идет при `temperature=0` (детерминированнее),
- лимит по токенам `max_new_tokens`,
- если суммаризатор не задан — можно вернуть только `context` и кандидатов (удобно для отладки retrieval).

---

## Почему это “Temporal RAG”, а не просто RAG

Обычный RAG решает: “что релевантно запросу вообще”.
Temporal RAG решает: “что релевантно запросу **на дату X**”.

Ключевые отличия:
1) **Temporal masking**: исключение будущего.
2) **Time bonus**: предпочтение свежего возле anchor_date.
3) **Hot-window контекст**: приоритет “последнего месяца” в промпте.
4) **Стабильность через ранги**: и RRF, и time bonus используют ранги, а не сырые скоры.
5) **Дедуп и кластеры с учетом времени**: near-duplicate объединение ограничено по day-diff.

---

## Минимальная “карта” основных сущностей

- `TemporalRAGIndexBuilder`:
  - BM25 (лексика)
  - E5 embeddings (семантика)
  - FAISS (поиск topN)

- `hybrid_retrieve_rrf`:
  - dense topN + bm25 topN
  - RRF fusion
  - + time bonus
  - + optional channel bonus

- `dedup_cluster_candidates_time`:
  - быстрый дедуп по hash(text_norm)+day
  - семантический union-find кластеров по cosine + day constraint

- `judge_filter_candidates`:
  - LLM-классификация релевантности 0/1/2

- `build_rag_context`:
  - hot-window selection + history
  - сортировка по датам
  - сборка промпта

- `rag_summarize` / `TemporalRAGPipeline.answer`:
  - финальное саммари по контексту
